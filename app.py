import streamlit as st
import os
from datasets import load_dataset
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.llms.base import LLM
from typing import Optional, List, Any
import google.generativeai as genai
import warnings
warnings.filterwarnings('ignore')

# Sayfa yapÄ±landÄ±rmasÄ±
st.set_page_config(
    page_title="Next.js RAG AsistanÄ± - Gemini 2.0",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Gemini LLM Wrapper
class GeminiLLM(LLM):
    model_name: str = "gemini-2.0-flash-exp"
    temperature: float = 0.7
    
    @property
    def _llm_type(self) -> str:
        return "gemini"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        try:
            model = genai.GenerativeModel(self.model_name)
            response = model.generate_content(
                prompt,
                generation_config=genai.GenerationConfig(
                    temperature=self.temperature,
                    max_output_tokens=2048,
                )
            )
            return response.text
        except Exception as e:
            return f"Hata oluÅŸtu: {str(e)}"
    
    @property
    def _identifying_params(self) -> dict:
        return {"model_name": self.model_name, "temperature": self.temperature}

# Ã–nbellek fonksiyonlarÄ±
@st.cache_resource
def load_and_prepare_data():
    """Veri setini yÃ¼kle ve hazÄ±rla"""
    with st.spinner("Veri seti yÃ¼kleniyor..."):
        # Hugging Face veri setini yÃ¼kle (ilk 100 dÃ¶kÃ¼man)
        dataset = load_dataset("ChavyvAkvar/Next.js-Dataset-Converted", split="train[:10]")
        
        # DÃ¶kÃ¼manlarÄ± hazÄ±rla
        documents = []
        for item in dataset:
            # Veri setindeki text alanÄ±nÄ± kullan
            text_content = item.get('text', '') or item.get('content', '') or str(item)
            
            if text_content and len(text_content.strip()) > 0:
                doc = Document(
                    page_content=text_content,
                    metadata={"source": "Next.js Dataset"}
                )
                documents.append(doc)
        
        st.success(f"âœ… {len(documents)} dÃ¶kÃ¼man yÃ¼klendi!")
        return documents

@st.cache_resource
def create_vector_store(_documents):
    """Vector store oluÅŸtur"""
    with st.spinner("Embeddings oluÅŸturuluyor ve vektÃ¶r veritabanÄ± hazÄ±rlanÄ±yor..."):
        # Text splitter
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100,
            length_function=len
        )
        
        # Metinleri parÃ§ala
        texts = text_splitter.split_documents(_documents)
        st.info(f"ğŸ“„ {len(texts)} metin parÃ§asÄ± oluÅŸturuldu")
        
        # Embeddings modeli
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'}
        )
        
        # FAISS vector store
        vectorstore = FAISS.from_documents(texts, embeddings)
        
        st.success("âœ… VektÃ¶r veritabanÄ± hazÄ±r!")
        return vectorstore

def get_qa_chain(vectorstore, gemini_api_key, temperature):
    """QA chain oluÅŸtur"""
    try:
        # Gemini API yapÄ±landÄ±r
        genai.configure(api_key=gemini_api_key)
        
        # Gemini LLM
        llm = GeminiLLM(temperature=temperature)
        
        # Prompt template
        prompt_template = """AÅŸaÄŸÄ±daki baÄŸlam bilgisini kullanarak soruyu yanÄ±tla. EÄŸer cevabÄ± baÄŸlamda bulamazsan, bilmiyorum de ve tahminde bulunma.

BaÄŸlam:
{context}

Soru: {question}

DetaylÄ± Cevap (TÃ¼rkÃ§e):"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # RetrievalQA chain
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        return qa_chain
    except Exception as e:
        st.error(f"QA Chain oluÅŸturulurken hata: {str(e)}")
        return None

def main():
    st.title("ğŸ¤– Next.js RAG AsistanÄ± - Gemini 2.0")
    st.markdown("*Google Gemini 2.0 Flash ile gÃ¼Ã§lendirilmiÅŸtir*")
    st.markdown("---")
    
    # Sidebar
    with st.sidebar:
        st.header("âš™ï¸ Ayarlar")
        
        # Gemini API Key
        gemini_api_key = st.text_input(
            "Google Gemini API Key",
            type="password",
            help="Google AI Studio'dan Ã¼cretsiz API key alabilirsiniz: https://aistudio.google.com/apikey"
        )
        
        # Temperature ayarÄ±
        temperature = st.slider(
            "Temperature (YaratÄ±cÄ±lÄ±k)",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.1,
            help="DÃ¼ÅŸÃ¼k deÄŸer: Daha deterministik, YÃ¼ksek deÄŸer: Daha yaratÄ±cÄ±"
        )
        
        st.markdown("---")
        st.markdown("### ğŸ“Š Model Bilgisi")
        st.info("**Model:** Gemini 2.0 Flash Exp\n\n**Ã–zellikler:**\n- HÄ±zlÄ± yanÄ±t\n- TÃ¼rkÃ§e desteÄŸi\n- GeliÅŸmiÅŸ anlama")
        
        st.markdown("### ğŸ”— Kaynaklar")
        st.markdown("- [Google AI Studio](https://aistudio.google.com)")
        st.markdown("- [Gemini API Docs](https://ai.google.dev)")
        st.markdown("- [Next.js Dataset](https://huggingface.co/datasets/ChavyvAkvar/Next.js-Dataset-Converted)")
    
    # Ana iÃ§erik
    if not gemini_api_key:
        st.warning("âš ï¸ LÃ¼tfen Google Gemini API Key'inizi yan panelden girin.")
        st.info("ğŸ‘‰ Ãœcretsiz API key almak iÃ§in: https://aistudio.google.com/apikey")
        
        # API key alma rehberi
        with st.expander("ğŸ“– API Key NasÄ±l AlÄ±nÄ±r?"):
            st.markdown("""
            1. https://aistudio.google.com/apikey adresine gidin
            2. Google hesabÄ±nÄ±zla giriÅŸ yapÄ±n
            3. "Create API Key" butonuna tÄ±klayÄ±n
            4. OluÅŸturulan API key'i kopyalayÄ±n
            5. Sol paneldeki alana yapÄ±ÅŸtÄ±rÄ±n
            """)
        return
    
    # Veri setini yÃ¼kle
    try:
        documents = load_and_prepare_data()
        
        # Vector store oluÅŸtur
        vectorstore = create_vector_store(documents)
        
        # QA chain oluÅŸtur
        qa_chain = get_qa_chain(vectorstore, gemini_api_key, temperature)
        
        if qa_chain is None:
            return
        
        st.markdown("---")
        st.success("âœ… Sistem hazÄ±r! Gemini 2.0 ile sorularÄ±nÄ±zÄ± sorabilirsiniz.")
        
        # Ã–rnek sorular
        with st.expander("ğŸ’¡ Ã–rnek Sorular"):
            st.markdown("""
            - Next.js'de server-side rendering nasÄ±l yapÄ±lÄ±r?
            - App Router ve Pages Router arasÄ±ndaki farklar nelerdir?
            - Next.js'de API route'larÄ± nasÄ±l oluÅŸturulur?
            - Static Site Generation (SSG) nedir?
            - Next.js'de middleware nasÄ±l kullanÄ±lÄ±r?
            """)
        
        # Soru-cevap bÃ¶lÃ¼mÃ¼
        st.markdown("### ğŸ’¬ Soru Sorun")
        
        question = st.text_area(
            "Sorunuz:",
            placeholder="Ã–rnek: Next.js'de server-side rendering nasÄ±l yapÄ±lÄ±r?",
            height=100
        )
        
        col1, col2, col3 = st.columns([1, 1, 4])
        with col1:
            ask_button = st.button("ğŸ” Ara", use_container_width=True, type="primary")
        with col2:
            clear_button = st.button("ğŸ—‘ï¸ Temizle", use_container_width=True)
        
        if clear_button:
            st.rerun()
        
        if ask_button and question:
            with st.spinner("ğŸ¤” Gemini 2.0 dÃ¼ÅŸÃ¼nÃ¼yor..."):
                try:
                    result = qa_chain({"query": question})
                    
                    # CevabÄ± gÃ¶ster
                    st.markdown("### ğŸ“ Cevap")
                    st.markdown(f"""
                    <div style="background-color: #f0f2f6; padding: 20px; border-radius: 10px; border-left: 5px solid #4CAF50;">
                    {result['result']}
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # Kaynak dÃ¶kÃ¼manlarÄ± gÃ¶ster
                    if 'source_documents' in result and result['source_documents']:
                        st.markdown("### ğŸ“š Kaynak DÃ¶kÃ¼manlar")
                        for i, doc in enumerate(result['source_documents'], 1):
                            with st.expander(f"ğŸ“„ Kaynak {i}"):
                                st.text(doc.page_content[:800] + "..." if len(doc.page_content) > 800 else doc.page_content)
                
                except Exception as e:
                    st.error(f"âŒ Hata oluÅŸtu: {str(e)}")
                    st.info("ğŸ’¡ API key'inizi kontrol edin veya birkaÃ§ saniye bekleyip tekrar deneyin.")
        
        elif ask_button and not question:
            st.warning("âš ï¸ LÃ¼tfen bir soru girin.")
    
    except Exception as e:
        st.error(f"âŒ Uygulama baÅŸlatÄ±lÄ±rken hata: {str(e)}")
        st.info("LÃ¼tfen requirements.txt dosyasÄ±ndaki tÃ¼m paketlerin kurulu olduÄŸundan emin olun.")

if __name__ == "__main__":
    main()